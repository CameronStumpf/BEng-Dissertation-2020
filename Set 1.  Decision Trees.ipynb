{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Organisation and Visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# SciKit Learn Feature Selection and Data Split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn import metrics  # metrics.mean_squared_error, metrics.r2_score\n",
    "\n",
    "# SciKit Learn Machine Learning Libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor  #Set 1, Model 1\n",
    "from sklearn.ensemble import RandomForestRegressor  #Set 1, Model 2\n",
    "\n",
    "# PyTorch and fast.ai Machine Learning Libraries\n",
    "import torch.nn as nn  #contains neural network layer methods\n",
    "import torch.nn.functional as F  #specifically convolutional layer methods\n",
    "import torch.optim as optim  #pytorch optimization algorithms\n",
    "from fastai.tabular import *  #Used in Set 2, Model 1 & 2 experiments\n",
    "\n",
    "# Miscellaneous\n",
    "import random  #random number generation used for testing\n",
    "import time  #timing individual sections of code\n",
    "from tqdm.notebook import tqdm  #progress bar for 'for loops'\n",
    "from datetime import datetime  #accurate time stamp indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_nasdaq100(path, tp):\n",
    "    filename = 'nasdaq100.csv'\n",
    "    data = pd.read_csv(path/filename) #pull data from filename\n",
    "    j=1\n",
    "    for i in reversed(tp):\n",
    "        k = str(i/390)\n",
    "        #CREATE STAGGERED TARGET COLUMN  (REGRESSION TARGET)\n",
    "        target_list = data[target].tolist()  #creates list from target column\n",
    "        target_list.extend([np.nan for x in range(i)])  #extends list with NaNs for specified time period\n",
    "        target_list = target_list[i:]  #cuts the list back to size\n",
    "\n",
    "        #CREATE UP/DOWN TARGET COLUMN  (CLASSIFICATION TARGET)\n",
    "        classification_list = []\n",
    "        for i in range(len(target_list)):\n",
    "            if target_list[i] >= data['NDX'][i]:  item = 1\n",
    "            else:  item = 0\n",
    "            classification_list.append(item)\n",
    "        \n",
    "        data.insert(0, 'Classification Target: +'+k+' Days', classification_list) #insert\n",
    "        data.insert(0+j, 'Regression Target: +'+k+' Days', target_list) #insert\n",
    "        j+=1 \n",
    "    return data.dropna()  #remove rows with NaN\n",
    "\n",
    "\n",
    "def feature_selection(data, num_features, mode):\n",
    "    #mode either classif or regress\n",
    "    #split full df into x and y, feature select, to df then rejoin  -  we lose data in column names, but that's ok\n",
    "    columns,index = data.columns.to_list(), data.index.values  #extract columns and index of x\n",
    "    if mode == \"classification\":\n",
    "        data = data.drop(data.columns[range(10,20)], axis=1)  #drop the regression targets\n",
    "        X,y = data.drop(data.columns[range(0,10)], axis=1), data[data.columns[range(0,10)]]\n",
    "        X = SelectKBest(mutual_info_classif, k=num_features).fit_transform(X,y.iloc[:,0])  #this reduces input features\n",
    "        \n",
    "    if mode == \"regression\":\n",
    "        data = data.drop(data.columns[range(0,10)], axis=1)  #drop the classification targets\n",
    "        X,y = data.drop(data.columns[range(0,10)], axis=1), data[data.columns[range(0,10)]]\n",
    "        X = SelectKBest(mutual_info_regression, k=num_features).fit_transform(X,y.iloc[:,0])  #this reduces input features\n",
    "    \n",
    "    X = pd.DataFrame(X, index=index)\n",
    "    data = pd.concat([y, X], axis=1, sort=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sequence_input(data, sequence_length, sequence_spacing):\n",
    "    df_main, df_x = data.iloc[:,:10], data.iloc[:,10:]\n",
    "    nan_unit = pd.DataFrame(np.nan, columns=df_x.columns, index=range(sequence_spacing)) #create nan row df\n",
    "    y_cols = df_main.columns.tolist()\n",
    "\n",
    "    for i in range(sequence_length):  #for i we create and concat a new temp_x\n",
    "        nan_block = nan_unit.iloc[:0, :]  #create \n",
    "        for j in range(i):   #create nan block to add to start of temp_x\n",
    "            nan_block = pd.concat([nan_block, nan_unit], ignore_index=True)\n",
    "        temp_x = pd.concat([nan_block, df_x], ignore_index=True)  #make overlength x\n",
    "        df_main = pd.concat([df_main, temp_x], axis=1, ignore_index=True)\n",
    "        df_cols = y_cols + list(range(len(df_main.columns)-10))\n",
    "        df_main.columns = df_cols\n",
    "    return df_main.dropna()\n",
    "\n",
    "   \n",
    "\n",
    "def split(data):   #split data into train, valid and test sets\n",
    "    data_train, data_test = data[:-5000], data[-5000:]\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def scale(data_train, data_test):\n",
    "    #set up normalization on train, apply to valid and test\n",
    "    train_x = data_train.iloc[:, 10:]  #pull x values from train data\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x) #fit scale model to train set\n",
    "    \n",
    "    data, scaled_data = [data_train, data_test], []\n",
    "    for df in data:\n",
    "        df_y, df_x = df.iloc[:, :10], df.iloc[:, 10:]  #splits data into y and x columns\n",
    "        columns,index = df_x.columns.to_list(), df_x.index.values  #extract columns and index of x\n",
    "        x_scaled = scaler.transform(df_x)  #returns np.array\n",
    "        df_x = pd.DataFrame(x_scaled, columns=columns, index=index)\n",
    "        df = pd.concat([df_y, df_x], axis=1, sort=False)\n",
    "        scaled_data.append(df)\n",
    "        \n",
    "    return scaled_data[0], scaled_data[1]  #train, valid, test\n",
    "\n",
    "\n",
    "def truncate(data, n_rows):\n",
    "    #REDUCE df_nasdaq100 INTO THE SMALLER dataframe        \n",
    "    data = data.iloc[-n_rows:]\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_Xy(data_train, data_test):\n",
    "    data_list = [data_train, data_test]\n",
    "    X_list, y_list = [], []\n",
    "    for data in data_list:  X_list.append(data.iloc[:, 10:]), y_list.append(data.iloc[:, :10])\n",
    "    return [X_list[0],y_list[0]],  [X_list[1],y_list[1]]\n",
    "\n",
    "\n",
    "\n",
    "#SCORE AND GRAPHING FUNCTIONS\n",
    "\n",
    "def score_classif(model, target, predicted): #score classification result\n",
    "    tested,correct = 0,0\n",
    "    for i in range(len(target)):\n",
    "        if   target[i] == predicted[i]: correct,tested = correct+1,tested+1\n",
    "        elif target[i] != predicted[i]: tested +=1\n",
    "    print(model + \" Classification Score: \", (correct/tested)*100)\n",
    "\n",
    "    \n",
    "def score_regress(target, prediction):  #target is as a series, prediciton is as an np.array\n",
    "    for i in range(1, len(target)):\n",
    "        score_list  = [metrics.r2_score(target, prediction),\n",
    "                       np.sqrt(metrics.mean_squared_error(target, prediction))]  # [r2, mse]\n",
    "    return score_list\n",
    "\n",
    "\n",
    "def plot_col(targ, pred, col):  #pring regression results\n",
    "    #plot specified columns in pred vs. targ\n",
    "    x_axis = list(range(len(df_test)))\n",
    "    # Data for plotting\n",
    "\n",
    "    for i in range(len(col)):\n",
    "        fig, a = plt.subplots()  #1,len(col))\n",
    "        a.plot(x_axis, targ.iloc[:, col[i]], label='NDX^ Target Value')\n",
    "        a.plot(x_axis, pred.iloc[:, col[i]], label='NDX^ Predicted Value')\n",
    "        a.set(xlabel='Test Dataset Index', ylabel='NDX^ Value',\n",
    "              title='Forecast: +'+str((tp[col[i]-1])/390)+' Days')\n",
    "        a.legend(loc='upper right')\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_row(targ, pred, row):\n",
    "    fig = go.Figure()\n",
    "    x_axis = ['NDX Base Truth']\n",
    "    for i in pred.columns.tolist()[1:]: x_axis.append(i.split(\" \", 2)[2])\n",
    "\n",
    "    for j in range(len(row)):\n",
    "        fig, a = plt.subplots() #1,len(row))\n",
    "        a.plot(x_axis, targ.iloc[row[j]], label='NDX^ Target Value')\n",
    "        a.plot(x_axis, pred.iloc[row[j]], label='NDX^ Predicted Value')\n",
    "        a.set(xlabel='Time (Days)', ylabel='NDX^ Value',\n",
    "              title='0.5-5 Day Forecast: Row Index: '+str(row[j]))\n",
    "        a.legend(loc='upper right')\n",
    "        a.set_xticklabels(x_axis, rotation=40, ha='right')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nasdaq100.csv'     # data file to be used\n",
    "path = pathlib.Path.home()/'OneDrive'/'19-20 3rd Yr MEng'/'Dissertation'/'Data'/'NASDAQ-100 II'\n",
    "#path = pathlib.Path.cwd()\n",
    "target = 'NDX'  #target column name to be predicted\n",
    "\n",
    "data_norm = True\n",
    "data_redu = True\n",
    "it_input_features = [20]\n",
    "it_data_rows = [30000]\n",
    "\n",
    "tp1  = [39, 78, 117, 156, 195, 234, 273, 312, 351, 390]  #10 models up to 1 day\n",
    "tp5  = [195, 390, 585, 780, 975, 1170, 1365, 1560, 1755, 1950]  #10 models up to 5 days\n",
    "tp10 = [390, 780, 1170, 1560, 1950, 2340, 2730, 3120, 3510, 3900]  #10 models up to 10 days\n",
    "\n",
    "tp = tp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pull_nasdaq100(path, tp)\n",
    "\n",
    "#graph data\n",
    "x_axis = list(range(len(df_full)))\n",
    "y_ndx = df_full['NDX']\n",
    "\n",
    "#targets\n",
    "fig = go.Figure()\n",
    "fig.add_shape(dict(type=\"line\",x0=len(df_full)-5460,y0=4600,x1=len(df_full)-5460,y1=5000,line=dict( color=\"Red\", width=1)))\n",
    "fig.add_trace(go.Scatter(x=x_axis, y=y_ndx, fill=None, mode='lines', name='NDX^ Price'))\n",
    "for column in df_full.columns.tolist()[10:20]:\n",
    "    fig.add_trace(go.Scatter(x=x_axis, y=df_full[column], fill=None, mode='lines', name=column))\n",
    "fig.update_layout(title=\"NASDAQ100 NDX^:  July 26th to December 22nd 2016\", xaxis_title=\"Time\", yaxis_title=\"NDX^ value\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#inputs\n",
    "fig = go.Figure()\n",
    "fig.add_shape(dict(type=\"line\",x0=len(df_full)-5460,y0=0,x1=len(df_full)-5460,y1=200,line=dict( color=\"Red\", width=1)))\n",
    "for column in df_full.columns.tolist()[20:]:\n",
    "    fig.add_trace(go.Scatter(x=x_axis, y=df_full[column], fill=None, mode='lines', name=column))\n",
    "fig.update_layout(title=\"NASDAQ100 NDX^:  July 26th to December 22nd 2016\", xaxis_title=\"Time\", yaxis_title=\"NDX^ value\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "## Set 1, Models 1 & 2:   Decision Tree Methods\n",
    "\n",
    "########################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## Set 1, Models 1 and 2\n",
    "#####\n",
    "\n",
    "tp = [195, 390, 585, 780, 975, 1170, 1365, 1560, 1755, 1950]  #10 models up to 5 days\n",
    "input_features = 20\n",
    "learning_rate = 0.1\n",
    "data_rows = 30000\n",
    "n_estimators = 100\n",
    "\n",
    "s1_m1 = []\n",
    "s1_m2 = []\n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "df_full = pull_nasdaq100(path, tp)  #import data with y_c and y_r cols\n",
    "df = feature_selection(df_full, input_features, 'regression')  #reduce columns\n",
    "df_train, df_test = split(df)  #split train/valid/test\n",
    "df_train, df_test = scale(df_train, df_test)  #normalize and scale data\n",
    "df_train = truncate(df_train, data_rows)\n",
    "train_R, test_R = split_Xy(df_train, df_test)\n",
    "[X_train,y_train], [X_test,y_test] = train_R, test_R\n",
    "\n",
    "input_list = df_train.columns.tolist()[10:]  #isolate list of inputs\n",
    "target_list = df_train.columns.tolist()[:10]  #list of target column names\n",
    "ndx_df = pd.DataFrame(df_full['NDX'])  #create df with just ndx\n",
    "ndx_df.rename(columns={\"NDX\": \"NDX Base Truth\"})  #rename ndx column\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "\n",
    "for n_estimators in tqdm([100], leave=True):\n",
    "    for learning_rate in [0.01]:\n",
    "        tag = \"n_estimators: \"+str(n_estimators)  #what experiment is this?\n",
    "\n",
    "        ###############################\n",
    "\n",
    "        #Set 1, Model 1\n",
    "        n_estimators = 200\n",
    "        learning_rate = 0.1\n",
    "        df_targ = pd.concat([ndx_df, y_test], axis=1).dropna()  #creates target dataframe, dropna to remove excess ndx\n",
    "        df_pred = df_targ.iloc[:, :1]  #creates empty prediction dataframe\n",
    "        df_score = pd.DataFrame(index=[\"R^2 Score\", \"Root Mean Square Error\"])  #create score df\n",
    "\n",
    "\n",
    "        for column in tqdm(target_list, leave=False):  #train for each target column\n",
    "            col_num = column.split(\" \", 2)[2]\n",
    "\n",
    "            model = GradientBoostingRegressor(loss='ls', learning_rate=learning_rate, n_estimators=n_estimators, subsample=1.0,\n",
    "                                              criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "                                              min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None, init=None, random_state=None, max_features=None,\n",
    "                                              alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated',\n",
    "                                              validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "            model.fit(X_train, y_train[column])   #fit on y column of this for loop\n",
    "            y_pred = model.predict(X_test)  #predict and release as an array\n",
    "            df_pred.insert(len(df_pred.columns), \"Regression Prediction: \"+col_num, y_pred) #join df_pred with predictions\n",
    "            score = score_regress(y_test[column], y_pred)  #score model  (series, np.array)\n",
    "            df_score.insert(len(df_score.columns), col_num, score )  #insert score into score df\n",
    "\n",
    "        s1_m1.append([df_targ, df_pred, df_score, tag])\n",
    "\n",
    "\n",
    "        ###############################\n",
    "\n",
    "        '''#Set 1, Model 2:  Random Forest\n",
    "        #n_estimators = \n",
    "        df_targ = pd.concat([ndx_df, y_test], axis=1).dropna()  #creates target dataframe, dropna to remove excess ndx\n",
    "        df_pred = df_targ.iloc[:, :1]  #just ndx column, will be added to later on\n",
    "        df_score = pd.DataFrame(index=[\"R^2 Score\", \"Mean Square Error\"])  #create score df\n",
    "\n",
    "        for column in tqdm(target_list, leave=False):\n",
    "            col_num = column.split(\" \", 2)[2]\n",
    "\n",
    "            model = RandomForestRegressor(n_estimators=n_estimators, criterion='mse', max_depth=None, min_samples_split=2,\n",
    "                                          min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                                          max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                          bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0,\n",
    "                                          warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "            model.fit(X_train, y_train[column])   #fit on y column of this for loop\n",
    "            y_pred = model.predict(X_test)  #predict and release as an array\n",
    "            df_pred.insert(len(df_pred.columns), \"Regression Prediction: \"+col_num, y_pred) #join df_pred with predictions\n",
    "            score = score_regress(y_test[column], y_pred)  #score model  (series, np.array)\n",
    "            df_score.insert(len(df_score.columns), col_num, score )  #insert score into score df\n",
    "\n",
    "        s1_m2.append([df_targ, df_pred, df_score, tag])  #returns df_pred, df_targ, and df_score\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ, df_pred, df_score, tag = s1_m2[1]\n",
    "df_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_targ1, df_pred1, df_score1 = s1_m1  #set 1 model 1\n",
    "#df_targ2, df_pred2, df_score2 = s1_m2  #set 1 model 2\n",
    "\n",
    "#I'm still sat here with df_targ, df_pred, df_score, plus all the \n",
    "#plot_row(df_targ1, df_pred1, [1000,2500,4000])  #plot row\n",
    "#plot_col(df_targ1, df_pred1, [1,5,10])  #plots predictions over time series\n",
    "#print(df_score1)\n",
    "\n",
    "print(\"\\n\\nSET 1 MODEL 1\\n\")  \n",
    "for i in range(9):\n",
    "    df_targ, df_pred, df_score, tag = s1_m1[i]\n",
    "    print(tag)\n",
    "    plot_row(df_targ, df_pred, [1000, 2500, 4500])  #plot row\n",
    "    plot_col(df_targ, df_pred, [1,5,10])\n",
    "    print(df_score)\n",
    "    print(\"\\n-----\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "## Set 1, Models 1 & 2\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Regression Set 1, Model 1:  Linear Regression\n",
    "\n",
    "#####\n",
    "## REGRESSION\n",
    "#####\n",
    "\n",
    "df_full = pull_nasdaq100(path, tp5)  #import data with y_c and y_r cols\n",
    "\n",
    "df = feature_selection(df_full, input_features, 'regression')  #reduce columns\n",
    "df_train, df_test = split(df)  #split train/valid/test\n",
    "df_train, df_test = scale(df_train, df_test)  #normalize and scale data\n",
    "df_train = truncate(df_train, data_rows)\n",
    "train_R, test_R = split_Xy(df_train, df_test)\n",
    "[X_train,y_train], [X_test,y_test] = train_R, test_R\n",
    "\n",
    "\n",
    "\n",
    "input_list = df_train.columns.tolist()[10:]\n",
    "target_list = df_train.columns.tolist()[:10]\n",
    "\n",
    "ndx_df = pd.DataFrame(df_full['NDX'])  #create df with just ndx\n",
    "ndx_df.rename(columns={\"NDX\": \"NDX Base Truth\"})  #rename ndx column\n",
    "df_targ = pd.concat([ndx_df, y_test], axis=1).dropna()  #creates target dataframe, dropna to remove excess ndx\n",
    "df_pred = df_targ.iloc[:, :1]  #just ndx column, will be added to later on\n",
    "df_score = pd.DataFrame(index=[\"R^2 Score\", \"Mean Square Error\"])  #create score df\n",
    "\n",
    "for column in tqdm(target_list):\n",
    "    col_num = column.split(\" \", 2)[2]\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)  #define model\n",
    "    \n",
    "    model.fit(X_train, y_train[column])   #fit on y column of this for loop\n",
    "    y_pred = model.predict(X_test)  #predict and release as an array\n",
    "    df_pred.insert(len(df_pred.columns), \"Regression Prediction: \"+col_num, y_pred) #join df_pred with predictions\n",
    "    score = score_regress(y_test[column], y_pred)  #score model  (series, np.array)\n",
    "    df_score.insert(len(df_score.columns), col_num, score )  #insert score into score df\n",
    "\n",
    "\n",
    "s1_m1 = [df_targ, df_pred, df_score]  #returns df_pred, df_targ, and df_score\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "#Regression Set 1, Model 1:  SGD Regressor\n",
    "#CREATE LIST OF MODELS, ONE PER ITEM IN TP\n",
    "input_list = df_train.columns.tolist()[10:]\n",
    "target_list = df_train.columns.tolist()[:10]\n",
    "\n",
    "ndx_df = pd.DataFrame(df_full['NDX'])  #create df with just ndx\n",
    "ndx_df.rename(columns={\"NDX\": \"NDX Base Truth\"})  #rename ndx column\n",
    "df_targ = pd.concat([ndx_df, y_test], axis=1).dropna()  #creates target dataframe, dropna to remove excess ndx\n",
    "df_pred = df_targ.iloc[:, :1]  #just ndx column, will be added to later on\n",
    "df_score = pd.DataFrame(index=[\"R^2 Score\", \"Mean Square Error\"])  #create score df\n",
    "\n",
    "for column in tqdm(target_list):\n",
    "    col_num = column.split(\" \", 2)[2]\n",
    "    \n",
    "    model = SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.0001, l1_ratio=0.15,\n",
    "                      fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0,\n",
    "                      epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01,\n",
    "                      power_t=0.25, early_stopping=False, validation_fraction=0.1,\n",
    "                      n_iter_no_change=5, warm_start=False, average=False)\n",
    "    \n",
    "    model.fit(X_train, y_train[column])   #fit on y column of this for loop\n",
    "    y_pred = model.predict(X_test)  #predict and release as an array\n",
    "    df_pred.insert(len(df_pred.columns), \"Regression Prediction: \"+column.split(\" \", 2)[2], y_pred) #join df_pred with predictions\n",
    "    score = score_regress(y_test[column], y_pred)  #score model  (series, np.array)\n",
    "    df_score.insert(len(df_score.columns), col_num, score )  #insert score into score df\n",
    "\n",
    "\n",
    "s1_m2 = [df_targ, df_pred, df_score]  #returns df_pred, df_targ, and df_score'''\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "'''###\n",
    "\n",
    "#Regression Set 1, Model 5:  Gradient Boosting\n",
    "#CREATE LIST OF MODELS, ONE PER ITEM IN TP\n",
    "input_list = df_train.columns.tolist()[10:]  #isolate list of inputs\n",
    "target_list = df_train.columns.tolist()[:10]  #isolate list of targets\n",
    "ndx_df = pd.DataFrame(df_full['NDX'])  #create df with just ndx\n",
    "ndx_df.rename(columns={\"NDX\": \"NDX Base Truth\"})  #rename ndx column\n",
    "df_targ = pd.concat([ndx_df, y_test], axis=1).dropna()  #creates target dataframe, dropna to remove excess ndx\n",
    "df_pred = df_targ.iloc[:, :1]  #just ndx column, will be added to later on\n",
    "df_score = pd.DataFrame(index=[\"R^2 Score\", \"RMSE\"])  #create score df\n",
    "\n",
    "for column in tqdm(target_list):\n",
    "    col_num = column.split(\" \", 2)[2]\n",
    "    \n",
    "    model = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                      min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None,\n",
    "                                      max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, \n",
    "                                      presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001,\n",
    "                                      ccp_alpha=0.0)\n",
    "    \n",
    "    model.fit(X_train, y_train[column])   #fit on y column of this for loop\n",
    "    y_pred = model.predict(X_test)  #predict and release as an array\n",
    "    df_pred.insert(len(df_pred.columns), \"Regression Prediction: \"+col_num, y_pred) #join df_pred with predictions\n",
    "    score = score_regress(y_test[column], y_pred)  #score model  (series, np.array)\n",
    "    df_score.insert(len(df_score.columns), col_num, score )  #insert score into score df\n",
    "\n",
    "s1_m5 = [df_targ, df_pred, df_score]  #returns df_pred, df_targ, and df_score\n",
    "\n",
    "\n",
    "\n",
    "###'''\n",
    "'''###\n",
    "\n",
    "#Regression Set 1, Model 6: AdaBoost\n",
    "input_list = df_train.columns.tolist()[10:]  #isolate list of inputs\n",
    "target_list = df_train.columns.tolist()[:10]  #isolate list of targets\n",
    "ndx_df = pd.DataFrame(df_full['NDX'])  #create df with just ndx\n",
    "ndx_df.rename(columns={\"NDX\": \"NDX Base Truth\"})  #rename ndx column\n",
    "df_targ = pd.concat([ndx_df, y_test], axis=1).dropna()  #creates target dataframe, dropna to remove excess ndx\n",
    "df_pred = df_targ.iloc[:, :1]  #just ndx column, will be added to later on\n",
    "df_score = pd.DataFrame(index=[\"R^2 Score\", \"Mean Square Error\"])  #create score df\n",
    "\n",
    "for column in tqdm(target_list):\n",
    "    col_num = column.split(\" \", 2)[2]\n",
    "    \n",
    "    model = AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "    \n",
    "    model.fit(X_train, y_train[column])   #fit on y column of this for loop\n",
    "    y_pred = model.predict(X_test)  #predict and release as an array\n",
    "    df_pred.insert(len(df_pred.columns), \"Regression Prediction: \"+col_num, y_pred) #join df_pred with predictions\n",
    "    score = score_regress(y_test[column], y_pred)  #score model  (series, np.array)\n",
    "    df_score.insert(len(df_score.columns), col_num, score )  #insert score into score df\n",
    "\n",
    "s1_m6 = [df_targ, df_pred, df_score]  #returns df_pred, df_targ, and df_score\n",
    "\n",
    "\n",
    "###'''\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "## CLASSIFICATION\n",
    "#####\n",
    "\n",
    "'''\n",
    "df_full = pull_nasdaq100(path, tp)  #import data with y_c and y_r cols\n",
    "\n",
    "df = feature_selection(df_full, input_features, 'classification')  #reduce columns\n",
    "df_train, df_test = split(df)  #split train/valid/test\n",
    "df_train, df_test = scale(df_train, df_test)  #normalize and scale data\n",
    "df_train = truncate(df_train, data_rows)\n",
    "train_C, test_C = split_Xy(df_train, df_test)\n",
    "[X_train,y_train], [X_test,y_test] = train_C, test_C\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "##################\n",
    "#Classification Model 0:  Logistic Regression\n",
    "\n",
    "Model0 = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight=None, random_state=None,\n",
    "                            solver='lbfgs', max_iter=100, multi_class='auto', verbose=0,\n",
    "                            warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "Model0.fit(X_train,y_train)\n",
    "y_pred0, y_test0 = Model0.predict(X_test).tolist(), y_test.values.tolist()\n",
    "score_classif(\"Logistic Regression\", y_test0, y_pred0)\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "#######################\n",
    "#Classification Model 1:  SGD Classifier\n",
    "            \n",
    "Model1 = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True,\n",
    "                       max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None,\n",
    "                       random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5,\n",
    "                       early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, \n",
    "                       class_weight=None, warm_start=False, average=False)\n",
    "\n",
    "Model1.fit(X_train,y_train)\n",
    "y_pred1, y_test1 = Model1.predict(X_test).tolist(), y_test.values.tolist()\n",
    "score_classif(\"SGD Regression\", y_test1, y_pred1)\n",
    "\n",
    "'''\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "## CLASSIFICATION\n",
    "#####\n",
    "'''\n",
    "\n",
    "df_full = pull_nasdaq100(path, tp)  #import data with y_c and y_r cols\n",
    "\n",
    "df = feature_selection(df_full, input_features, 'classification')  #reduce columns\n",
    "df_train, df_test = split(df)  #split train/valid/test\n",
    "df_train, df_test = scale(df_train, df_test)  #normalize and scale data\n",
    "df_train = truncate(df_train, data_rows)\n",
    "train_C, test_C = split_Xy(df_train, df_test)\n",
    "[X_train,y_train], [X_test,y_test] = train_C, test_C\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "##################\n",
    "#Classification Model 0:  Single decision tree method\n",
    "\n",
    "Model0 = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None,\n",
    "                                min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                max_features=None, random_state=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None,\n",
    "                                presort='deprecated', ccp_alpha=0.0)\n",
    "\n",
    "Model0.fit(X_train,y_train)\n",
    "y_pred0, y_test0 = Model0.predict(X_test).tolist(), y_test.values.tolist()\n",
    "score_classif(\"SGD Regression\", y_test0, y_pred0)\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "#######################\n",
    "#Classification Model 1:  Random Forest\n",
    "            \n",
    "Model1 = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None,\n",
    "                                min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None, bootstrap=True, oob_score=False,\n",
    "                                n_jobs=None, random_state=None, verbose=0, warm_start=False,\n",
    "                                class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "Model1.fit(X_train,y_train)\n",
    "y_pred1, y_test1 = Model1.predict(X_test).tolist(), y_test.values.tolist()\n",
    "score_classif(\"SGD Regression\", y_test1, y_pred1)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "#####################\n",
    "#Classification Model 2:  Gradient Boosting\n",
    "\n",
    "Model2 = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100,\n",
    "                                    subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                                    min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                                    min_impurity_decrease=0.0, min_impurity_split=None,init=None,\n",
    "                                    random_state=None, max_features=None, verbose=0,\n",
    "                                    max_leaf_nodes=None, warm_start=False, presort='deprecated',\n",
    "                                    validation_fraction=0.1, n_iter_no_change=None, tol=0.0001,\n",
    "                                    ccp_alpha=0.0)\n",
    "\n",
    "Model2.fit(X_train,y_train)\n",
    "y_pred2, y_test2 = Model2.predict(X_test).tolist(), y_test.values.tolist()\n",
    "score_classif(\"SGD Regression\", y_test2, y_pred2)\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "\n",
    "######################\n",
    "#Classification Model 3:  AdaBoost\n",
    "\n",
    "Model3 = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0,\n",
    "                            algorithm='SAMME.R', random_state=None)\n",
    "\n",
    "Model3.fit(X_train,y_train)\n",
    "y_pred3, y_test3 = Model3.predict(X_test).tolist(), y_test.values.tolist()\n",
    "score_classif(\"SGD Regression\", y_test3, y_pred3)\n",
    "\n",
    "'''\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "\n",
    "#######################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Single Row\n",
    "[df_targ, df_pred, df_score] = s1_m2\n",
    "\n",
    "for i in [0, 4999]:\n",
    "    plot_row(df_targ, df_pred, i)\n",
    "\n",
    "for i in [0.5, 5]:\n",
    "    j = tp.index(i*390)\n",
    "    plot_col(df_targ, df_pred, j)\n",
    "    \n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [1,5,10]\n",
    "for i in range(len(col)):\n",
    "    print(tp[col[i]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai v1",
   "language": "python",
   "name": "fastai_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
